---
title: "p8105_hw3_yw3996"
author: "Yiying Wu"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## 1. Instacart data
load the data from the p8105.datasets using:
```{r,include=TRUE,results = 'hide',warning = FALSE, message = FALSE}
library(p8105.datasets)
data("instacart")
instacart=instacart
```
### Description of the dataset

* This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. 
* Variables include 
  
  * `order_id`: order identifier
  * `product_id`: product identifier
  * `add_to_cart_order`: order in which each product was added to cart
  * `reordered`: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
  * `user_id`: customer identifier
  * `eval_set`: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval_set)
  * `order_number`: the order sequence number for this user (1=first, n=nth)
  * `order_dow`: the day of the week on which the order was placed
  * `order_hour_of_day`: the hour of the day on which the order was placed
  * `days_since_prior_order`: days since the last order, capped at 30, NA if order_number=1
  * `product_name`: name of the product
  * `aisle_id`: aisle identifier
  * `department_id`: department identifier
  * `aisle`: the name of the aisle
  * `department`: the name of the department

### Count the number of aisles and the aisles with most items ordered
Count the number of aisles in the dataset:
```{r}
n_distinct(instacart$aisle)
```

Count the number of orders in each aisle and sort by descending order:
```{r}
instacart |>
  group_by(aisle) |>
  summarize(n_obs = n())|>
  arrange(desc(n_obs))
```
Therefore, there are `r n_distinct(instacart$aisle)` aisles. The most items are ordered from fresh vegetables aisles.

### Plot 1 
the number of items ordered in each aisle (aisles with more than 10000 items ordered)

```{r}
instacart |>
  group_by(aisle) |>
  summarize(n_obs = n()) |>
  filter(n_obs > 10000) |>
  mutate(aisle = fct_reorder(aisle, desc(n_obs))) |>
  ggplot(aes(x = aisle, y = n_obs)) + 
  geom_bar(stat = "identity") +  # Use geom_bar for a bar chart
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

### Table 1 
the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits” with the number of times each item is ordered
```{r}
instacart |> 
  filter(aisle %in% c("baking ingredients", 
                      "dog food care", 
                      "packaged vegetables fruits")) |>
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> 
  arrange(desc(n)) |>
  knitr::kable()
```

### Table 2 
the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(mean_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = "order_dow", 
    values_from = "mean_hour")|>
  knitr::kable(digits = 2)
```
## 2. BRFSS data
load the data from the p8105.datasets using:
```{r,include=TRUE,results = 'hide',warning = FALSE, message = FALSE}
data("BRFSS")
brfss=brfss_smart2010
```

### data cleaning

* format the data to use appropriate variable names;
* focus on the “Overall Health” topic
* include only responses from “Excellent” to “Poor”
* organize responses as a factor taking levels ordered from “Poor” to “Excellent”

list all the value of response column:
```{r}
brfss=brfss|>
  janitor::clean_names()|>
  mutate(data_value=as.double(data_value))
unique(brfss$response)
```
The responses should be included are "Excellent", "Very good",  "Good", "Fair", "Poor".

The order should be "Poor", "Fair", "Good", "Very good", "Excellent"

```{r}
brfss=brfss|>
  filter(
    topic=="Overall Health",
    response %in% c("Poor", "Fair", "Good", 
                    "Very good", "Excellent"))|>
  mutate(
    response=factor(response, levels=c("Poor", "Fair", "Good", 
                                       "Very good", "Excellent"))
  )
```

### states observed at 7 or more locations in 2002 or in 2010 

states were observed at 7 or more locations in 2002 are: 
```{r}
brfss|>
  filter(year==2002)|>
  group_by(locationabbr)|>
  summarize(n_obs = n_distinct(locationdesc))|>
  filter(n_obs>=7)
```
CT, FL, MA, NC, NJ, PA were observed at 7 or more locations in 2002.

states were observed at 7 or more locations in 2010 are:
```{r}
brfss|>
  filter(year==2010)|>
  group_by(locationabbr)|>
  summarize(n_obs = n_distinct(locationdesc))|>
  filter(n_obs>=7)
```

CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA were observed at 7 or more locations in 2010.

### Construct a dataset with Excellent responses including year, state, and a variable that averages the data_value across locations within a state

```{r}
dat = brfss|>
  filter(response=="Excellent")|>
  group_by(year, locationabbr)|>
  summarize(data_value_mean=mean(data_value, na.rm = TRUE))
```
### spaghetti plot
```{r, fig.width = 8}
dat|>
  rename("state"="locationabbr")|>
  ggplot(aes(x = year, y = data_value_mean, 
             color =state )) + 
  geom_point() + geom_line() + 
  theme(legend.position = "bottom")+
  guides(color = guide_legend(nrow = 5))
```

### two-panel plot
two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.
```{r, fig.width = 9}
plot_2006=
  brfss |>
  filter(locationabbr=="NY",year==2006)|>
  ggplot(aes(x =locationdesc , y = data_value,fill=response)) + 
  geom_bar(position="dodge",stat="identity") +  
  labs(title = "Distribution of data_value in NY State in 2006") +
  theme(axis.text.x = element_text(angle = 15, hjust = 1),legend.position = "none")

plot_2010=
  brfss |>
  filter(locationabbr=="NY",year==2010)|>
  ggplot(aes(x =locationdesc , y = data_value,fill=response)) + 
  geom_bar(position="dodge",stat="identity") +  
  labs(title = "Distribution of data_value in NY State in 2010") +
  theme(axis.text.x = element_text(angle = 15, hjust = 1))

plot_2006 + plot_2010
```

## 3. Accelerometer data
import dataset
```{r, include=TRUE,results = 'hide',warning = FALSE, message = FALSE}
demographic=read_csv("./data/nhanes_covar.csv",skip = 4)|>
  janitor::clean_names()
accelerometer=read_csv("./data/nhanes_accel.csv")|>
  janitor::clean_names()
```

### data cleaning
clean `demographic` dataset:

* include all originally observed variables; 
* exclude participants less than 21 years of age, and those with missing demographic data; and 
* encode data with reasonable variable classes
```{r}
demographic=demographic|>
  drop_na()|>
  filter(age>=21)|>
  mutate(
     sex=case_match(
       sex,
       1~"male",
       2~"female"),
     education=case_match(
       education,
       1~"Less than high school",
       2~"High school equivalent",
       3~"More than high school"
     )
  )
```

clean `accelerometer` data

* pivot longer
```{r}
accelerometer=accelerometer|>
  drop_na()|>
  pivot_longer(
    min1:min1440,
    names_to = "time", 
    values_to = "mims_value")|>
  separate(time,into=c("min","minute"),3)|>
  select(-min)|>
  mutate(minute=as.integer(minute))
```

join `accelerometer` with `demographic`  
```{r}
dat_3=
  inner_join(demographic,accelerometer,by="seqn")
```

table for the number of men and women in each education category, and create a visualization of the age distributions for men and women in each education category. Comment on these items.